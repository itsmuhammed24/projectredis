# Projet d'Analyse et de D√©tection d'Anomalies sur les Transactions Financi√®res avec Redis

## Description du projet
Ce projet vise √† d√©velopper une solution permettant l'analyse et la d√©tection d'anomalies dans des transactions financi√®res en utilisant **Redis** comme base de donn√©es en m√©moire, **FastAPI** pour l'exposition des donn√©es via API, **Celery** pour l'automatisation des t√¢ches, et **Streamlit** pour la visualisation des r√©sultats. L'objectif est d'offrir une approche rapide et efficace pour stocker, analyser et surveiller les transactions, avec un accent particulier sur la d√©tection des comportements frauduleux.

---

## Architecture de la solution

L'architecture de la solution repose sur les composants suivants :

1. **Stockage des donn√©es avec Redis :**
   - Importation des transactions √† partir d'un fichier CSV.
   - Stockage sous forme de hachage (hashes) pour un acc√®s rapide et efficace.
   - Utilisation d'UUID pour assurer l'unicit√© des transactions.
   - Application d'un TTL (Time-To-Live) pour simuler une gestion de session utilisateur.

2. **Exposition des donn√©es via FastAPI :**
   - Mise √† disposition d'API REST pour r√©cup√©rer et filtrer les transactions.
   - Gestion des requ√™tes utilisateur en temps r√©el.

3. **D√©tection d'anomalies avec Machine Learning :**
   - Utilisation de l'algorithme Isolation Forest pour identifier les transactions suspectes.
   - Stockage des anomalies d√©tect√©es dans Redis pour un suivi ult√©rieur.
   - Automatisation de la d√©tection via Celery.

4. **Visualisation avec Streamlit :**
   - Tableaux et graphiques interactifs pour analyser les tendances des transactions.
   - Affichage des anomalies d√©tect√©es en temps r√©el.

---

## Pr√©requis

Avant d'ex√©cuter le projet, assurez-vous d'avoir install√© les d√©pendances requises :

1. **Python 3.x** (v√©rifiez avec `python --version`)
2. **Redis** (install√© via Homebrew sur macOS : `brew install redis`)
3. **Modules Python n√©cessaires :**
   ```bash
   pip install -r requirements.txt
   ```

---

## Installation

1. **Cloner le projet :**
   ```bash
   git clone https://github.com/mon-projet-redis.git
   cd mon-projet-redis
   ```

2. **Cr√©er un environnement virtuel et l'activer :**
   ```bash
   python -m venv myenv
   source myenv/bin/activate  # macOS/Linux
   myenv\Scripts\activate  # Windows
   ```

3. **Installer les d√©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

4. **Lancer Redis :**
   ```bash
   brew services start redis  # Pour macOS
   redis-server  # Autre m√©thode manuelle
   ```

---

## Ex√©cution du projet

### 1. Stockage des donn√©es dans Redis
Ex√©cuter le script pour importer les transactions depuis le fichier CSV :

```bash
python store_transactions.py
```
*Sortie attendue :*  
`Transactions stock√©es avec succ√®s.`

---

### 2. D√©marrage de l'API FastAPI
Lancer le serveur API pour r√©cup√©rer les donn√©es stock√©es :

```bash
uvicorn api:app --reload
```
Acc√©der √† la documentation interactive via Swagger UI :  
üëâ `http://127.0.0.1:8000/docs`

---

### 3. Lancer la d√©tection des anomalies
Ex√©cuter le script d'analyse des anomalies avec Celery :

```bash
celery -A tasks worker --loglevel=info
```

---

### 4. V√©rification des anomalies d√©tect√©es dans Redis
Pour afficher les anomalies d√©tect√©es :

```bash
redis-cli
SMEMBERS anomalies
```

---

### 5. Lancer le tableau de bord Streamlit
Ex√©cuter le tableau de bord pour visualiser les donn√©es et anomalies :

```bash
streamlit run dashboard.py
```
Acc√©der au tableau de bord via :  
üëâ `http://localhost:8501`

---

## Surveillance des logs

Il est possible de suivre les logs des diff√©rents services pour ajuster les performances :

- **API FastAPI :**  
  ```bash
  uvicorn api:app --reload --log-level debug
  ```
- **Celery :**  
  ```bash
  celery -A tasks worker --loglevel=info
  ```
- **Streamlit :**  
  ```bash
  streamlit run dashboard.py --logger.level=debug
  ```

---

## Axes d'am√©lioration

Plusieurs axes d'am√©lioration sont envisag√©s :

- **Automatisation de l'ingestion des donn√©es** avec Kafka ou RabbitMQ pour assurer un flux continu.
- **Int√©gration de sources externes** via du scraping ou des API.
- **Migration vers le cloud** pour assurer la scalabilit√© et la r√©silience.
- **S√©curisation des donn√©es** avec OAuth 2.0 et conformit√© RGPD/PCI-DSS.

---

## Sch√©ma de l'Architecture

Le sch√©ma suivant illustre l'architecture du projet :

![Architecture du Projet](img/img.png)

